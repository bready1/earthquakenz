{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy import UTCDateTime\n",
    "from obspy.clients.fdsn import Client as FDSN_Client\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from progress.bar import Bar\n",
    "from tqdm.notebook import tqdm\n",
    "from multiprocessing import Pool\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load list of quakes\n",
    "quakelist=pd.read_csv('data/quakelist1.txt',sep=\"|\") \n",
    "quakelist.columns = quakelist.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "\n",
    "#load stationlist\n",
    "stationlist=pd.read_csv('data/station_h_data.txt',sep=\"|\")\n",
    "stationlist.columns = stationlist.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "rand_inds=np.array(range(len(quakelist)))\n",
    "np.random.shuffle(rand_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = FDSN_Client(\"GEONET\")\n",
    "client_nrt = FDSN_Client(\"https://service-nrt.geonet.org.nz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # turning off warnings because otherwise obspy spams warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_to_quake(tt):\n",
    "    #returns True if the time is within an hour (either side) of a quake in quakelist, otherwise False\n",
    "    quaketime_diffs=[UTCDateTime(quaketime)-UTCDateTime(tt) for quaketime in quakelist['time'].to_numpy()]\n",
    "    if any(np.abs(quaketime_diffs)<3600): #if any quake happens within an hour either side of the time\n",
    "        too_close=True\n",
    "    else:\n",
    "        too_close=False\n",
    "    return too_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_stream(st1):\n",
    "    #Function for determining whether a stream st1 is good data\n",
    "    #Is it of at least 3 traces long\n",
    "    #Does it have the first 3 traces channels as \"HHZ\", \"HHE\" or \"HHN\"\n",
    "    #it have the correct number of points, 32000 or 32001\n",
    "    #test for any std==0 \n",
    "    stream_good=1\n",
    "    if len(st1)<3: #is it at least 3 long\n",
    "        return False\n",
    "    else:\n",
    "        filled=[0,0,0]\n",
    "        for tr_ind in range(3): #testing that it has the 3 channels\n",
    "            if st1[tr_ind].stats.channel==\"HHZ\":\n",
    "                fill_ind=0\n",
    "            elif st1[tr_ind].stats.channel==\"HHN\":\n",
    "                fill_ind=1\n",
    "            elif st1[tr_ind].stats.channel==\"HHE\":\n",
    "                fill_ind=2\n",
    "            else: \n",
    "                return False\n",
    "            \n",
    "            filled[fill_ind]=1\n",
    "            if not (st1[tr_ind].stats.npts==32000 or st1[tr_ind].stats.npts==32001): #testing it has the correct npts\n",
    "#             if not (st1[tr_ind].stats.npts==240001 or st1[tr_ind].stats.npts==240000):\n",
    "\n",
    "                return False\n",
    "\n",
    "            if np.std(st1[tr_ind].data)==0: #making sure its actually changing in time\n",
    "                return False\n",
    "    \n",
    "        if not filled ==[1,1,1]: #making sure it had all the channels\n",
    "            return False\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rand_times(n_samples,wait_time=3600):\n",
    "    #generates n_sample random times (floats)\n",
    "    #each of these times is not closer than wait_time (default 1 hour) on \n",
    "    #either side of one of the quakes in quakelist\n",
    "    quaketimes=np.array(([float(UTCDateTime(quaketime)) for quaketime in quakelist['time'].to_numpy()]))\n",
    "    max_time=max(quaketimes)\n",
    "    min_time=min(quaketimes)\n",
    "    rand_times=np.zeros(n_samples)\n",
    "    ii=0\n",
    "    \n",
    "    while ii<n_samples:\n",
    "        t_temp=np.random.uniform(min_time,max_time)\n",
    "        if all(np.abs(quaketimes-t_temp)>wait_time):\n",
    "            rand_times[ii]=t_temp\n",
    "            ii+=1\n",
    "#             if ii%20==0:\n",
    "#                 print(ii)\n",
    "\n",
    "    return rand_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quake_savefail_write(fail_id,data_dir,reason=''):\n",
    "    savefail=open(data_dir+'quake_savefail.txt','a')\n",
    "    savefail.write(fail_id+' '+ reason)\n",
    "    savefail.write(\"\\n\")\n",
    "    savefail.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='/media/peter/data/earthquakenz/data/test/' # data to save quakebloks to\n",
    "station_list1=['APZ', 'BFZ', 'BKZ', 'CTZ',  'DSZ', 'EAZ', 'FOZ',\n",
    "       'FWVZ', 'GLKZ', 'GRZ', 'GVZ', 'HAZ', 'HIZ', 'INZ', 'JCZ', 'KHEZ',\n",
    "       'KHZ', 'KNZ',  'LBZ', 'LTZ', 'MLZ', 'MQZ',\n",
    "       'MRZ', 'MSZ', 'MWZ', 'MXZ', 'NNZ', 'ODZ', 'OPRZ', 'OPZ', 'OTVZ',\n",
    "       'OUZ', 'OXZ', 'PUZ', 'PXZ', 'PYZ', 'QRZ', 'RATZ', 'RIZ', \n",
    "       'RTZ', 'SYZ', 'THZ', 'TLZ', 'TMVZ', 'TOZ', 'TRVZ', 'TSZ', 'TUZ',\n",
    "        'VRZ', 'WAZ', 'WCZ', 'WEL', 'WHVZ', 'WHZ', 'WIZ', 'WKZ',\n",
    "       'WSRZ', 'WVZ'] #list of seismograph stations to use\n",
    "#this list of stations took a long time to arrive at, a lot of the stations have bad data and so if the \n",
    "#station list was too long then there would be not many quakes/other time sections where they all work\n",
    "#but you do want to have a substantial portion of them working, I think this is a good middle ground.\n",
    "\n",
    "# rand_times=np.load(data_dir+'rand_times.npy') #loaded rand_times\n",
    "\n",
    "\n",
    "def parallel_download_noquake(t_start,step=10):\n",
    "    #This function takes a time t_start, and attempts to download a 5 minute time section\n",
    "    #(10 second padding on either side) from all the stations \n",
    "    #in station_list1, from the HHZ,HHN, and HHE sensors. If the data from each seismograph is there and good\n",
    "    #it adds the stream data to a blok. The step parameter tells it how many steps to take when saving \n",
    "    #the waveform in time to the blok. It then saves the blok to a .npy file.\n",
    "    #If anything bad happens, like the download fails, the quake happened before the construction of a station,\n",
    "    #the stream data is bad (for any stream), then the function stops and doesn't save a blok\n",
    "    \n",
    "    #also it has a cool progress bar for each blok, and can be run in parallel\n",
    "    \n",
    "    \n",
    "    t_duration=5*60 #duration of the time section (without 10 second padding)\n",
    "    samplerate=100 #the assumed sample rate of the traces\n",
    "    blok=np.zeros((int(np.ceil(t_duration*samplerate/step)),3,len(station_list1))) #empty blok to save\n",
    "    starttime=10 \n",
    "    start_times=np.zeros((3,len(station_list1)))\n",
    "    timeid=str(t_start) #t_start as a string, to be used as a label\n",
    "#     print('Starting: '+ str(UTCDateTime(timeid)))\n",
    "    print('Starting: '+ str((timeid)))\n",
    "    for station_ind in tqdm(range(len(station_list1)),leave=False,desc=timeid): #for each station (with progressbar)\n",
    "        #check that the station existed at the time\n",
    "        if stationlist[stationlist['station']==station_list1[station_ind]]['starttime'].to_numpy()<UTCDateTime(t_start-10):\n",
    "            attempts=0\n",
    "            downloaded_stream=0\n",
    "            while attempts <4: #4 attempts to download\n",
    "                try:\n",
    "    #                 st= client.get_waveforms(\"NZ\",stationlist['station'][station_ind],\"*\", \"HHZ,HHN,HHE,HH1,HH2\", t-t_before, t+t_after ,minimumlength=t_before,attach_response=True)\n",
    "                    # st is the stream which is downloaded, for the station\n",
    "                    warnings.filterwarnings('ignore')\n",
    "                    st= client.get_waveforms(\"NZ\",station_list1[station_ind] ,\"*\", \"HH?\", UTCDateTime(t_start-10), UTCDateTime(t_start+5*60+10),attach_response=True)\n",
    "                    warnings.filterwarnings('default')\n",
    "                    downloaded_stream=1 #successfully downloaded!\n",
    "#                   \n",
    "                    break\n",
    "                except:\n",
    "                    attempts+=1 #incriment attempts if it failed\n",
    "            \n",
    "            if downloaded_stream: \n",
    "                if good_stream(st): #test stream is good\n",
    "                    #also test that there are the correct channels for the traces\n",
    "                    filled=[0,0,0]\n",
    "                    for tr_ind in range(3):\n",
    "                        if st[tr_ind].stats.channel==\"HHZ\":\n",
    "                            fill_ind=0\n",
    "                        elif st[tr_ind].stats.channel==\"HHN\":\n",
    "                            fill_ind=1\n",
    "                        elif st[tr_ind].stats.channel==\"HHE\":\n",
    "                            fill_ind=2\n",
    "                        else:\n",
    "                            print('SOMETHING WRONG WITH THE HH* INDS')\n",
    "                        #add to the blok! (if everything is good)\n",
    "                        blok[:,fill_ind,station_ind]=st[tr_ind].data[starttime*samplerate:int(t_duration*samplerate)+starttime*samplerate:step]\n",
    "                        filled[fill_ind]=1\n",
    "                        start_times[fill_ind,station_ind]=float(st[tr_ind].stats.starttime) #start time of the trace\n",
    "\n",
    "                    if not filled== [1,1,1]: #this tells us if we dont have a HHZ, HHN, HHE stream\n",
    "                        print('NOT FILLED!')\n",
    "                        return\n",
    "                        \n",
    "                    if station_ind>0: #this makes sure that the start times are close enough together\n",
    "                        #its alright if they're about 1 sample out, but more than that and it might be bad\n",
    "                        if np.max(np.abs(start_times[:,station_ind]-start_times[:,station_ind-1]))>1.5/samplerate:\n",
    "                            print('Something off with start times around at ' + timeid+str(station_ind)+', '+station_list1[station_ind])\n",
    "                            print(start_times[:,station_ind]-start_times[:,station_ind-1])\n",
    "                            print(np.max(np.abs(start_times[:,station_ind]-start_times[:,station_ind-1])))\n",
    "                else:\n",
    "                    return\n",
    "            else: \n",
    "                return\n",
    "    save_attempts=0\n",
    "    while save_attempts<5: #5 attempts to save this blok\n",
    "        try:  \n",
    "            np.save(data_dir+'noquakes/bloks1/'+timeid+'.npy',blok) #save blok to .npy file\n",
    "            print('Ending: '+ str((timeid)))\n",
    "\n",
    "            break\n",
    "        except:\n",
    "            save_attempts+=1 #incriminent attempts if it failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: 1480296830.13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32933a8b4e9741649d1d89f151f9bb1c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: 1583858017.55\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89b237aba014988a4b7a6d5da1da926"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: 1538155240.77\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8518cc6d18544add9ae6e47e7757c855"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: 1585274280.62\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55187b237235480f8246bf1db49bebb9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending: 1585274280.62\n"
     ]
    }
   ],
   "source": [
    "for ii in range(10,14):\n",
    "    parallel_download_noquake(rand_times[ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='/media/peter/data/earthquakenz/data/test/' # data to save quakebloks to\n",
    "station_list1=['APZ', 'BFZ', 'BKZ', 'CTZ',  'DSZ', 'EAZ', 'FOZ',\n",
    "       'FWVZ', 'GLKZ', 'GRZ', 'GVZ', 'HAZ', 'HIZ', 'INZ', 'JCZ', 'KHEZ',\n",
    "       'KHZ', 'KNZ',  'LBZ', 'LTZ', 'MLZ', 'MQZ',\n",
    "       'MRZ', 'MSZ', 'MWZ', 'MXZ', 'NNZ', 'ODZ', 'OPRZ', 'OPZ', 'OTVZ',\n",
    "       'OUZ', 'OXZ', 'PUZ', 'PXZ', 'PYZ', 'QRZ', 'RATZ', 'RIZ', \n",
    "       'RTZ', 'SYZ', 'THZ', 'TLZ', 'TMVZ', 'TOZ', 'TRVZ', 'TSZ', 'TUZ',\n",
    "        'VRZ', 'WAZ', 'WCZ', 'WEL', 'WHVZ', 'WHZ', 'WIZ', 'WKZ',\n",
    "       'WSRZ', 'WVZ'] #list of seismograph stations to use\n",
    "#this list of stations took a long time to arrive at, a lot of the stations have bad data and so if the \n",
    "#station list was too long then there would be not many quakes/other time sections where they all work\n",
    "#but you do want to have a substantial portion of them working, I think this is a good middle ground.\n",
    "\n",
    "t_before_rand=(15+np.random.rand(len(quakelist))*6-3)*60 #for each quake we take a section starting \n",
    "#between 12 and 18 mins before the quaketime \n",
    "\n",
    "def parallel_download_quake(quake_ind,step=10,save_fail=True):\n",
    "    #This function takes a time t_start, and attempts to download a 5 minute time section\n",
    "    #(10 second padding on either side) from all the stations \n",
    "    #in station_list1, from the HHZ,HHN, and HHE sensors. If the data from each seismograph is there and good\n",
    "    #it adds the stream data to a blok. The step parameter tells it how many steps to take when saving \n",
    "    #the waveform in time to the blok. It then saves the blok to a .npy file.\n",
    "    #If anything bad happens, like the download fails, the quake happened before the construction of a station,\n",
    "    #the stream data is bad (for any stream), then the function stops and doesn't save a blok\n",
    "    save_dir=data_dir+'quakes/bloks1/'\n",
    "    #also it has a cool progress bar for each blok, and can be run in parallel\n",
    "    quakeid=quakelist['eventid'][quake_ind]\n",
    "    t_start=float(UTCDateTime(quakelist['time'][quake_ind]))-t_before_rand[quake_ind] #time that the\n",
    "    t_duration=5*60 #duration of the time section (without 10 second padding)\n",
    "    samplerate=100 #the assumed sample rate of the traces\n",
    "    blok=np.zeros((int(np.ceil(t_duration*samplerate/step)),3,len(station_list1))) #empty blok to save\n",
    "    starttime=10 \n",
    "    start_times=np.zeros((3,len(station_list1)))\n",
    "    timeid=str(t_start) #t_start as a string, to be used as a label\n",
    "    print('Starting: '+ str((quakeid)))\n",
    "    for station_ind in tqdm(range(len(station_list1)),leave=False,desc=quakeid): #for each station (with progressbar)\n",
    "        #check that the station existed at the time\n",
    "        if stationlist[stationlist['station']==station_list1[station_ind]]['starttime'].to_numpy()<UTCDateTime(t_start-10):\n",
    "            attempts=0\n",
    "            downloaded_stream=0\n",
    "            while attempts <4: #4 attempts to download\n",
    "                try:\n",
    "    #                 st= client.get_waveforms(\"NZ\",stationlist['station'][station_ind],\"*\", \"HHZ,HHN,HHE,HH1,HH2\", t-t_before, t+t_after ,minimumlength=t_before,attach_response=True)\n",
    "                    # st is the stream which is downloaded, for the station\n",
    "                    warnings.filterwarnings('ignore')\n",
    "                    st= client.get_waveforms(\"NZ\",station_list1[station_ind] ,\"*\", \"HH?\", UTCDateTime(t_start-10), UTCDateTime(t_start+5*60+10),attach_response=True)\n",
    "                    warnings.filterwarnings('default')\n",
    "                    downloaded_stream=1 #successfully downloaded!\n",
    "#                   \n",
    "                    break\n",
    "                except:\n",
    "                    attempts+=1 #incriment attempts if it failed\n",
    "            \n",
    "            if downloaded_stream: \n",
    "                if good_stream(st): #test stream is good\n",
    "                    #also test that there are the correct channels for the traces\n",
    "                    filled=[0,0,0]\n",
    "                    for tr_ind in range(3):\n",
    "                        if st[tr_ind].stats.channel==\"HHZ\":\n",
    "                            fill_ind=0\n",
    "                        elif st[tr_ind].stats.channel==\"HHN\":\n",
    "                            fill_ind=1\n",
    "                        elif st[tr_ind].stats.channel==\"HHE\":\n",
    "                            fill_ind=2\n",
    "                        else:\n",
    "                            print('SOMETHING WRONG WITH THE HH* INDS')\n",
    "                        #add to the blok! (if everything is good)\n",
    "                        blok[:,fill_ind,station_ind]=st[tr_ind].data[starttime*samplerate:int(t_duration*samplerate)+starttime*samplerate:step]\n",
    "                        filled[fill_ind]=1\n",
    "                        start_times[fill_ind,station_ind]=float(st[tr_ind].stats.starttime) #start time of the trace\n",
    "\n",
    "                    if not filled== [1,1,1]: #this tells us if we dont have a HHZ, HHN, HHE stream\n",
    "                        print('NOT FILLED!')\n",
    "                        quake_savefail_write(quakeid+' '+station_list1[station_ind],save_dir,'notfilled')\n",
    "                        return\n",
    "                        \n",
    "                    if station_ind>0: #this makes sure that the start times are close enough together\n",
    "                        #its alright if they're about 1 sample out, but more than that and it might be bad\n",
    "                        if np.max(np.abs(start_times[:,station_ind]-start_times[:,station_ind-1]))>1.5/samplerate:\n",
    "                            print('Something off with start times around at ' + timeid+str(station_ind)+', '+station_list1[station_ind])\n",
    "                            print(start_times[:,station_ind]-start_times[:,station_ind-1])\n",
    "                            print(np.max(np.abs(start_times[:,station_ind]-start_times[:,station_ind-1])))\n",
    "                else:\n",
    "                    quake_savefail_write(quakeid+' '+station_list1[station_ind],save_dir,'notgoodstream')\n",
    "                    return\n",
    "            else:\n",
    "                quake_savefail_write(quakeid+' '+station_list1[station_ind],save_dir,'notdownloaded')\n",
    "                return\n",
    "        else:\n",
    "            return\n",
    "    save_attempts=0\n",
    "    while save_attempts<5: #5 attempts to save this blok\n",
    "        try:  \n",
    "            np.save(save_dir+quakeid+'.npy',blok) #save blok to .npy file\n",
    "            print('Ending: '+ str((quakeid)))\n",
    "\n",
    "            break\n",
    "        except:\n",
    "            save_attempts+=1 #incriminent attempts if it failed\n",
    "    if save_attempts==5:\n",
    "        quake_savefail_write(quakeid+' '+station_list1[station_ind],save_dir,'savingfailed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: 2016p934797\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c66c6d45b584ccd8d4f70fc565dc55d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: 2016p868025\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403cd4b3736f4ed8b8c7c8bce058da7f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending: 2016p868025\n",
      "Starting: 2016p935569\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efcee3b5cc4048abb0af7fef53912f30"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: 2017p066354\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d452a0b3ecb9436f875d03ff0afe3618"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: 2017p071831\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697551d636a94705befd4aeabbd27b5e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: 2016p867831\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3f38cbb97944978cc9e7b5440b127c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending: 2016p867831\n"
     ]
    }
   ],
   "source": [
    "for ii in range(4,10):\n",
    "    parallel_download_quake(ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_bloks(blok_dir,bloknorm_dir):\n",
    "    #normalised all the bloks in blok_dir, and saves in the new directory bloknorm_dir\n",
    "    file_list=os.listdir(blok_dir)#list all the files in the blok dir\n",
    "    \n",
    "    for filename in file_list: #for each block\n",
    "        if filename[-4:]=='.npy': #make sure its actually a data file, not a txt or something else\n",
    "            blok=np.load(blok_dir+filename) #load unnormalised blok\n",
    "            blok_norm=np.zeros(np.shape(blok))\n",
    "            blok_std=np.std(blok,axis=0) #std for each channel\n",
    "            blok_mean=np.mean(blok,axis=0) #mean for each channel\n",
    "            for ii in range(3): #for each channel\n",
    "                for station_ind in range(blok.shape[2]): #for each station\n",
    "                    #normalise the blok\n",
    "                    blok_norm[:,ii,station_ind]=(blok[:,ii,station_ind]-blok_mean[ii,station_ind])/blok_std[ii,station_ind]\n",
    "            np.save(bloknorm_dir+filename,blok_norm) #save the normalised blok in bloknorm_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
